name: Model Pipeline

on:
  workflow_dispatch:
    inputs:
      steps:
        description: 'Pipeline steps to execute (comma-separated)'
        required: false
        default: 'train,eval'
      skip:
        description: 'Steps to skip (comma-separated)'
        required: false
        default: ''
      config:
        description: 'Pipeline configuration'
        required: false
        default: 'default'
      epochs:
        description: 'Training epochs (if training included)'
        required: false
        default: '10'
      job_id:
        description: 'Job ID from bot'
        required: false
      command:
        description: 'Original command'
        required: false
      user:
        description: 'User who triggered'
        required: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision scikit-learn pandas numpy matplotlib
        # Add your ML pipeline dependencies here
    
    - name: Display pipeline parameters
      run: |
        echo "ğŸ”„ Starting Model Pipeline"
        echo "Pipeline Steps: ${{ inputs.steps }}"
        echo "Skip Steps: ${{ inputs.skip }}"
        echo "Configuration: ${{ inputs.config }}"
        echo "Epochs: ${{ inputs.epochs }}"
        echo "Job ID: ${{ inputs.job_id }}"
        echo "Original Command: ${{ inputs.command }}"
        echo "Triggered by: ${{ inputs.user }}"
    
    - name: Parse pipeline configuration
      run: |
        echo "ğŸ“‹ Parsing pipeline configuration..."
        python -c "
import json

# Parse steps and skip lists
steps_str = '${{ inputs.steps }}'
skip_str = '${{ inputs.skip }}'

requested_steps = [s.strip() for s in steps_str.split(',') if s.strip()]
skip_steps = [s.strip() for s in skip_str.split(',') if s.strip()]

# Available pipeline steps
available_steps = ['train', 'eval', 'test', 'deploy', 'validate']

# Filter out skipped steps
pipeline_steps = [step for step in requested_steps if step in available_steps and step not in skip_steps]

pipeline_config = {
    'requested_steps': requested_steps,
    'skip_steps': skip_steps,
    'pipeline_steps': pipeline_steps,
    'config': '${{ inputs.config }}',
    'epochs': int('${{ inputs.epochs }}') if '${{ inputs.epochs }}' else 10
}

print(f'Pipeline execution plan:')
print(f'  Requested: {requested_steps}')
print(f'  Skipped: {skip_steps}')
print(f'  Will execute: {pipeline_steps}')

with open('pipeline_config.json', 'w') as f:
    json.dump(pipeline_config, f, indent=2)
"
    
    - name: Initialize pipeline environment
      run: |
        echo "ğŸ—ï¸ Initializing pipeline environment..."
        mkdir -p pipeline/{data,models,results,logs}
        echo "Environment initialized âœ…"
    
    - name: Execute training step
      run: |
        echo "ğŸ§  Executing training step..."
        python -c "
import json
import time
import random
from datetime import datetime

# Load pipeline config
with open('pipeline_config.json', 'r') as f:
    config = json.load(f)

if 'train' not in config['pipeline_steps']:
    print('â­ï¸  Training step skipped')
    exit(0)

print('ğŸš€ Starting training phase...')

epochs = config['epochs']
print(f'Training for {epochs} epochs...')

# Simulate training progress
training_results = {
    'step': 'train',
    'epochs': epochs,
    'start_time': datetime.utcnow().isoformat(),
    'metrics_history': []
}

for epoch in range(1, epochs + 1):
    time.sleep(1)  # Simulate training time
    
    # Simulate realistic training metrics
    loss = 1.0 * (0.85 ** epoch) + random.uniform(0, 0.1)
    accuracy = min(0.95, 0.6 + (epoch / epochs) * 0.3 + random.uniform(0, 0.05))
    
    epoch_metrics = {
        'epoch': epoch,
        'loss': round(loss, 4),
        'accuracy': round(accuracy, 4),
        'learning_rate': 0.001 * (0.95 ** (epoch // 5))  # Learning rate decay
    }
    
    training_results['metrics_history'].append(epoch_metrics)
    print(f'Epoch {epoch}/{epochs} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')

# Final training results
training_results.update({
    'end_time': datetime.utcnow().isoformat(),
    'final_loss': training_results['metrics_history'][-1]['loss'],
    'final_accuracy': training_results['metrics_history'][-1]['accuracy'],
    'model_path': 'pipeline/models/trained_model.pt',
    'status': 'completed'
})

with open('pipeline/results/training_results.json', 'w') as f:
    json.dump(training_results, f, indent=2)

print(f'âœ… Training completed! Final accuracy: {training_results[\"final_accuracy\"]:.3f}')
"
    
    - name: Execute evaluation step
      run: |
        echo "ğŸ“Š Executing evaluation step..."
        python -c "
import json
import time
import random
from datetime import datetime

# Load pipeline config
with open('pipeline_config.json', 'r') as f:
    config = json.load(f)

if 'eval' not in config['pipeline_steps']:
    print('â­ï¸  Evaluation step skipped')
    exit(0)

print('ğŸ” Starting evaluation phase...')

# Check if training was performed (for continuity)
training_results = None
try:
    with open('pipeline/results/training_results.json', 'r') as f:
        training_results = json.load(f)
    base_accuracy = training_results['final_accuracy']
    print(f'Using trained model with accuracy: {base_accuracy:.3f}')
except:
    base_accuracy = 0.85  # Default if no training step
    print('Using pre-trained model for evaluation')

# Simulate evaluation
time.sleep(2)

eval_results = {
    'step': 'eval',
    'start_time': datetime.utcnow().isoformat(),
    'base_model_accuracy': base_accuracy,
    'evaluation_metrics': {}
}

# Generate evaluation metrics
metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']
for metric in metrics:
    # Vary around base accuracy with some noise
    if metric == 'accuracy':
        value = base_accuracy + random.uniform(-0.02, 0.02)
    else:
        value = base_accuracy + random.uniform(-0.05, 0.03)
    
    eval_results['evaluation_metrics'][metric] = max(0, min(1, round(value, 4)))

eval_results.update({
    'end_time': datetime.utcnow().isoformat(),
    'test_samples': 2000,
    'inference_time_ms': round(random.uniform(15, 40), 2),
    'status': 'completed'
})

with open('pipeline/results/evaluation_results.json', 'w') as f:
    json.dump(eval_results, f, indent=2)

print('âœ… Evaluation completed!')
for metric, value in eval_results['evaluation_metrics'].items():
    print(f'   {metric}: {value:.4f}')
"
    
    - name: Execute testing step
      run: |
        echo "ğŸ§ª Executing testing step..."
        python -c "
import json
import time
import random
from datetime import datetime

# Load pipeline config
with open('pipeline_config.json', 'r') as f:
    config = json.load(f)

if 'test' not in config['pipeline_steps']:
    print('â­ï¸  Testing step skipped')
    exit(0)

print('ğŸ”¬ Starting testing phase...')

# Simulate comprehensive testing
time.sleep(2)

test_results = {
    'step': 'test',
    'start_time': datetime.utcnow().isoformat(),
    'test_suites': {}
}

# Define test suites
test_suites = {
    'smoke_tests': {'total': 5, 'description': 'Basic functionality tests'},
    'integration_tests': {'total': 8, 'description': 'End-to-end pipeline tests'},
    'performance_tests': {'total': 3, 'description': 'Performance benchmark tests'},
    'regression_tests': {'total': 12, 'description': 'Model behavior consistency tests'}
}

for suite_name, suite_info in test_suites.items():
    print(f'Running {suite_name}...')
    time.sleep(1)
    
    total_tests = suite_info['total']
    # Simulate high success rate (90-95%)
    passed_tests = total_tests - random.randint(0, max(1, total_tests // 10))
    failed_tests = total_tests - passed_tests
    
    suite_result = {
        'total_tests': total_tests,
        'passed_tests': passed_tests,
        'failed_tests': failed_tests,
        'success_rate': round((passed_tests / total_tests) * 100, 1),
        'description': suite_info['description']
    }
    
    test_results['test_suites'][suite_name] = suite_result
    
    status_icon = 'âœ…' if failed_tests == 0 else 'âš ï¸'
    print(f'   {status_icon} {suite_name}: {passed_tests}/{total_tests} passed ({suite_result[\"success_rate\"]}%)')

# Overall test summary
total_tests_run = sum(suite['total_tests'] for suite in test_results['test_suites'].values())
total_passed = sum(suite['passed_tests'] for suite in test_results['test_suites'].values())
total_failed = total_tests_run - total_passed

test_results.update({
    'end_time': datetime.utcnow().isoformat(),
    'summary': {
        'total_tests': total_tests_run,
        'total_passed': total_passed,
        'total_failed': total_failed,
        'overall_success_rate': round((total_passed / total_tests_run) * 100, 1),
        'status': 'passed' if total_failed <= 2 else 'failed'  # Allow some minor failures
    }
})

with open('pipeline/results/test_results.json', 'w') as f:
    json.dump(test_results, f, indent=2)

print(f'\\nâœ… Testing completed!')
print(f'   Total tests: {total_tests_run}')
print(f'   Passed: {total_passed}')
print(f'   Failed: {total_failed}')
print(f'   Success rate: {test_results[\"summary\"][\"overall_success_rate\"]}%')

if test_results['summary']['status'] == 'failed':
    print('âŒ Too many test failures - pipeline marked as unstable')
"
    
    - name: Execute validation step
      run: |
        echo "âœ… Executing validation step..."
        python -c "
import json
import time
from datetime import datetime

# Load pipeline config
with open('pipeline_config.json', 'r') as f:
    config = json.load(f)

if 'validate' not in config['pipeline_steps']:
    print('â­ï¸  Validation step skipped')
    exit(0)

print('ğŸ” Starting validation phase...')

# Load previous results for validation
validation_results = {
    'step': 'validate',
    'start_time': datetime.utcnow().isoformat(),
    'validations': {},
    'issues_found': []
}

# Validate training results
try:
    with open('pipeline/results/training_results.json', 'r') as f:
        training_data = json.load(f)
    
    training_valid = training_data['final_accuracy'] > 0.7
    validation_results['validations']['training'] = {
        'valid': training_valid,
        'accuracy': training_data['final_accuracy'],
        'threshold': 0.7
    }
    
    if not training_valid:
        validation_results['issues_found'].append('Training accuracy below threshold')
    
    print(f'   Training validation: {\"âœ…\" if training_valid else \"âŒ\"} (accuracy: {training_data[\"final_accuracy\"]:.3f})')
    
except:
    print('   âš ï¸  No training results to validate')

# Validate evaluation results
try:
    with open('pipeline/results/evaluation_results.json', 'r') as f:
        eval_data = json.load(f)
    
    eval_valid = eval_data['evaluation_metrics']['accuracy'] > 0.75
    validation_results['validations']['evaluation'] = {
        'valid': eval_valid,
        'metrics': eval_data['evaluation_metrics']
    }
    
    if not eval_valid:
        validation_results['issues_found'].append('Evaluation accuracy below expected')
    
    print(f'   Evaluation validation: {\"âœ…\" if eval_valid else \"âŒ\"}')
    
except:
    print('   âš ï¸  No evaluation results to validate')

# Validate test results
try:
    with open('pipeline/results/test_results.json', 'r') as f:
        test_data = json.load(f)
    
    tests_valid = test_data['summary']['overall_success_rate'] > 85
    validation_results['validations']['testing'] = {
        'valid': tests_valid,
        'success_rate': test_data['summary']['overall_success_rate']
    }
    
    if not tests_valid:
        validation_results['issues_found'].append('Test success rate below minimum')
    
    print(f'   Testing validation: {\"âœ…\" if tests_valid else \"âŒ\"} ({test_data[\"summary\"][\"overall_success_rate\"]}% success)')
    
except:
    print('   âš ï¸  No test results to validate')

# Overall validation status
all_valid = all(v.get('valid', False) for v in validation_results['validations'].values())
validation_results.update({
    'end_time': datetime.utcnow().isoformat(),
    'overall_status': 'passed' if all_valid else 'failed',
    'issues_count': len(validation_results['issues_found'])
})

with open('pipeline/results/validation_results.json', 'w') as f:
    json.dump(validation_results, f, indent=2)

print(f'\\nğŸ“‹ Validation Summary:')
print(f'   Status: {\"âœ… Passed\" if all_valid else \"âŒ Failed\"}')
print(f'   Issues found: {len(validation_results[\"issues_found\"])}')

if validation_results['issues_found']:
    print('   Issues:')
    for issue in validation_results['issues_found']:
        print(f'     - {issue}')
"
    
    - name: Generate pipeline report
      run: |
        echo "ğŸ“„ Generating pipeline report..."
        python -c "
import json
import os
from datetime import datetime

# Load pipeline configuration
with open('pipeline_config.json', 'r') as f:
    config = json.load(f)

# Load all step results
step_results = {}
result_files = [
    ('training', 'pipeline/results/training_results.json'),
    ('evaluation', 'pipeline/results/evaluation_results.json'),
    ('testing', 'pipeline/results/test_results.json'),
    ('validation', 'pipeline/results/validation_results.json')
]

for step_name, file_path in result_files:
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            step_results[step_name] = json.load(f)

# Generate comprehensive report
report = f'''# Model Pipeline Report

**Job ID:** ${{ inputs.job_id }}
**Command:** \`${{ inputs.command }}\`
**Triggered by:** ${{ inputs.user }}
**Completed:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}

## Pipeline Configuration
- **Steps Requested:** {', '.join(config['requested_steps'])}
- **Steps Executed:** {', '.join(config['pipeline_steps'])}
- **Steps Skipped:** {', '.join(config['skip_steps']) if config['skip_steps'] else 'None'}
- **Configuration:** {config['config']}

## Step Results

'''

# Add results for each executed step
if 'training' in step_results:
    training = step_results['training']
    report += f'''### ğŸ§  Training Step
- **Status:** âœ… Completed
- **Epochs:** {training['epochs']}
- **Final Accuracy:** {training['final_accuracy']:.4f}
- **Final Loss:** {training['final_loss']:.4f}
- **Model Path:** {training['model_path']}

'''

if 'evaluation' in step_results:
    evaluation = step_results['evaluation']
    report += f'''### ğŸ“Š Evaluation Step
- **Status:** âœ… Completed
- **Test Samples:** {evaluation.get('test_samples', 'N/A')}
- **Inference Time:** {evaluation.get('inference_time_ms', 'N/A')} ms

**Metrics:**
'''
    for metric, value in evaluation['evaluation_metrics'].items():
        report += f'- **{metric.upper()}:** {value:.4f}\\n'
    report += '\\n'

if 'testing' in step_results:
    testing = step_results['testing']
    report += f'''### ğŸ§ª Testing Step
- **Status:** {\"âœ… Passed\" if testing['summary']['status'] == 'passed' else \"âŒ Failed\"}
- **Total Tests:** {testing['summary']['total_tests']}
- **Passed:** {testing['summary']['total_passed']}
- **Failed:** {testing['summary']['total_failed']}
- **Success Rate:** {testing['summary']['overall_success_rate']}%

**Test Suites:**
'''
    for suite_name, suite_data in testing['test_suites'].items():
        status_icon = 'âœ…' if suite_data['failed_tests'] == 0 else 'âš ï¸'
        report += f'- {status_icon} **{suite_name.replace(\"_\", \" \").title()}**: {suite_data[\"passed_tests\"]}/{suite_data[\"total_tests\"]} ({suite_data[\"success_rate\"]}%)\\n'
    report += '\\n'

if 'validation' in step_results:
    validation = step_results['validation']
    report += f'''### âœ… Validation Step
- **Status:** {\"âœ… Passed\" if validation['overall_status'] == 'passed' else \"âŒ Failed\"}
- **Issues Found:** {validation['issues_count']}

'''
    if validation['issues_found']:
        report += '**Issues Identified:**\\n'
        for issue in validation['issues_found']:
            report += f'- âŒ {issue}\\n'
        report += '\\n'

# Overall pipeline status
all_steps_successful = True
if 'testing' in step_results and step_results['testing']['summary']['status'] == 'failed':
    all_steps_successful = False
if 'validation' in step_results and step_results['validation']['overall_status'] == 'failed':
    all_steps_successful = False

report += f'''## Pipeline Summary

**Overall Status:** {\"âœ… SUCCESS\" if all_steps_successful else \"âŒ FAILED\"}

**Steps Executed:** {len(config['pipeline_steps'])}
**Artifacts Generated:**
'''

# List artifacts
artifact_files = [
    'training_results.json',
    'evaluation_results.json', 
    'test_results.json',
    'validation_results.json',
    'pipeline_config.json'
]

for artifact in artifact_files:
    if any(os.path.exists(f'pipeline/results/{artifact}') or artifact == 'pipeline_config.json' for _ in [True]):
        report += f'- {artifact}\\n'

report += f'''
**Workflow URL:** [${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

---
*Generated by Model Pipeline Workflow*
'''

with open('pipeline_report.md', 'w') as f:
    f.write(report)
    
print('âœ… Pipeline report generated')
"
    
    - name: Upload pipeline artifacts
      uses: actions/upload-artifact@v3
      with:
        name: pipeline-artifacts-${{ inputs.job_id }}
        path: |
          pipeline/
          pipeline_report.md
          pipeline_config.json
    
    - name: Pipeline summary
      run: |
        echo "ğŸ‰ Model pipeline workflow completed!"
        echo "Check the artifacts for comprehensive results."
        echo "Workflow URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        
        # Check if any critical step failed
        python -c "
import json
import sys
import os

failed_steps = []

# Check test results
if os.path.exists('pipeline/results/test_results.json'):
    with open('pipeline/results/test_results.json', 'r') as f:
        test_data = json.load(f)
    if test_data['summary']['status'] == 'failed':
        failed_steps.append('testing')

# Check validation results  
if os.path.exists('pipeline/results/validation_results.json'):
    with open('pipeline/results/validation_results.json', 'r') as f:
        validation_data = json.load(f)
    if validation_data['overall_status'] == 'failed':
        failed_steps.append('validation')

if failed_steps:
    print(f'âŒ Pipeline completed with failures in: {', '.join(failed_steps)}')
    sys.exit(1)
else:
    print('âœ… Pipeline completed successfully!')
"